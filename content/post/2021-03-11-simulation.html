---
title: '[Intro to Computational Thinking] Introduction to Computational Thinking via Simulation with R'
author: Christopher Gandrud
date: '2021-03-17'
slug: optional-r-basics-3
weight: 4
categories:
  - R
  - install
tags: [R, simulation, distributed, functions]
Categories: []
Description: ''
Tags: [R, simulation, distributed, functions]
bibliography: main.bib
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/d3/d3.min.js"></script>
<script src="/rmarkdown-libs/dagre/dagre-d3.min.js"></script>
<link href="/rmarkdown-libs/mermaid/dist/mermaid.css" rel="stylesheet" />
<script src="/rmarkdown-libs/mermaid/dist/mermaid.slim.min.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/chromatography/chromatography.js"></script>
<script src="/rmarkdown-libs/DiagrammeR-binding/DiagrammeR.js"></script>


<blockquote>
<p>‚ÄúWhat I cannot create, I do not understand‚Äù ‚Äì <a href="https://archives.caltech.edu/pictures/1.10-29.jpg">Richard Feynman</a></p>
</blockquote>
<blockquote>
<p>‚ÄúIt is immoral to collect data before simulating the hypothesized data generating process.‚Äù ‚Äì Andrew Gelman (what I remember of a comment to the Stanford Causal Inference Seminar in 2020).</p>
</blockquote>
<div id="lesson-preview" class="section level2">
<h2>üìù Lesson Preview</h2>
<ul>
<li><p><a href="#motivation">Why simulate?</a></p></li>
<li><p><a href="#workflow">Simple simulation workflow</a></p></li>
<li><p><a href="#documenting">Documenting Functions in RMonte Carlo with apply and parallelization</a></p></li>
<li><p><a href="debugging-intro">Debugging in R</a></p></li>
<li><p>Throughout, we will include examples of how to plot with base R and ggplot2</p></li>
</ul>
</div>
<div id="motivation" class="section level2">
<h2>Motivation: Why simulate?</h2>
<p>We‚Äôve covered many of the basics of programming in R. Let‚Äôs bring many of these concepts together to help us better understand our research problems, even before we gather any data. Let‚Äôs simulate some <strong>data generating processes (DGP)</strong>.</p>
<p>A data generating process is the process that creates the data in a data set. These processes can include, for example, both the social processes that result in migration data and even unexpected (or undesired) issues in the sampling procedure like non-random study attrition.</p>
<p>Wait, why are we talking about simulating data generating processes in an introduction to R course? There are two sets of reasons.</p>
<p>First, computational generative modeling‚Äìsimulating data generating processes with a computer‚Äìcan be a key tool for understanding and evaluating the statistical methods you are using to understand a given data generating process <span class="citation">(<a href="#ref-gelmanhillaki2021" role="doc-biblioref">Gelman, Hill, and Vehtari 2021, 76</a>)</span>. Will your chosen statistical model identify the effect you are interested, even if you completely understand the DGP? This is surprisingly often not the case, especially for non-trivial data generating processes. Simulation allows you to stress test your chosen models, with a given assumed DGP, and compare the appropriateness of different models for this DGP.</p>
<p>In Gelman‚Äôs strong view stated above, if your identification strategy can‚Äôt recover effects in known‚Äìsimulated data‚Äìyou shouldn‚Äôt waste time and money collecting real world data.</p>
<p>The second reason we are covering statistical simulation, and most importantly for this class, is that simulation requires you to work your new skills as an R programmer. Because the DGP you are working with are probabilistic, you will need to repeat the simulations with the same data generating process to understand the statistical properties. This is called the <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo Method</a>. So you will need to write functions, run them efficiently, e.g.¬†sometimes in parallel, transform large amounts of data, and plot the results.</p>
</div>
<div id="workflow" class="section level2">
<h2>Simple simulation workflow</h2>
<p>A typical simulation workflow looks something like this:</p>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="DiagrammeR html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"\ngraph TB\n  A[Hypothesise DGP]-->AA[Identification strategies]\n  AA-->B[Functions for 1 draw & analysis]\n  B-->C[Function to repeat for n draws]\n  B-- Debug -->B\n  C-->D[Plot simulations]\n  C-- Debug -->C \n  D-- Debug -->D\n  D-- Iterate -->A\n"},"evals":[],"jsHooks":[]}</script>
<p>We start by hypothesizing what the data generating process we care about might be. Then we propose one or candidate strategies to identify effects in this data generating process, e.g.¬†regression, matching, difference in differences. We begin implementing the simulation fo this DGP-identification strategy by creating function(s) for simulating and analyzing the data for one simulation. Once we have the simulation correctly working for one draw, we create a function(s) to make <span class="math inline">\(n\)</span> draws. We then (usually) examine the results by plotting quantities of interest from the simulations, such as the distribution of some estimate or the error from some ground truth across simulations. Frequently, we learn something unexpected by the end either about our understanding of the data generating process or our candidate identification strategies. So we often need to repeat the process.</p>
<p>Notice that most steps often involve some debugging. We‚Äôll dig debugging into that <a href="#debugging-intro">below</a>. But let‚Äôs look at a simple ideal case.</p>
<div id="example-1-simple-ab-test-discrete-probabilities" class="section level3">
<h3>Example 1: Simple A/B Test Discrete probabilities</h3>
<p>Imagine that we want to run an A/B test‚Äìrandomised control trial‚Äìa new treatment to the status quo control treatment. We are interested in the effect of the treatment on whether or not individuals in the study do some action‚Äìe.g.¬†vote in an election.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> This is a discrete variable. Can we use a difference of means test to estimate the treatment effect?</p>
<p>Let‚Äôs start by hypothesizing what the data generating process could be. Imagine that based on prior research we know that their is a baseline probability of the action with the control of 0.1:</p>
<p><span class="math display">\[Y \sim \mathrm{Binomial}(0.1)\]</span> To simulate one draw from this binomial distribution in R use:</p>
<pre class="r"><code>control_y_1 &lt;- rbinom(n = 1, size = 1, prob = 0.1)
control_y_1</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Let‚Äôs walk through this code. <code>rbinom()</code> is the function to randomly (<code>r</code>) draw from the binomial distribution (<code>binom</code>). <code>n</code> is the number of draws for this simulation, <code>size</code> is the ‚Äúnumber of trials,‚Äù basically it means that you want to the ‚Äúsuccess‚Äù outcome to be 1 and 0 otherwise. <code>prob</code> is the probability of ‚Äúsuccess.‚Äù</p>
<p>It‚Äôs unlikely that we will run an A/B test with only one participant in the control group, let‚Äôs go up to 2,000,000<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> and plot the results.</p>
<pre class="r"><code># number of observations variable to ensure consistency across sims
n_obs &lt;- 2e6
control_prob &lt;- 0.1

control_y &lt;- rbinom(n = n_obs, size = 1, prob = control_prob)
hist(control_y)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<hr />
</div>
<div id="side-note-histograms-with-ggplot2" class="section level3">
<h3>Side note: Histograms with ggplot2</h3>
<p><a href="https://ggplot2.tidyverse.org/">ggplot2</a> is a very powerful way to create graphics in R. Here is an example of how you would create a histogram with ggplot2.</p>
<pre class="r"><code># ggplot2 is a member of the &quot;tidyverse&quot; set of packages
xfun::pkg_attach2(&quot;tidyverse&quot;)

# I personally enjoy the simplicity of the linedraw theme
theme_set(theme_linedraw())

# ggplot cannot accept a vector, like hist. 
# Instead you need to convert to a data frame or the tidyverse equivalent &quot;tibble&quot;
control_y_tbl &lt;- as_tibble(control_y)

# Plot and change the x-axis label
ggplot(control_y_tbl, aes(value)) + # as_tibble names the column &quot;value&quot; by default
  geom_histogram() + 
  xlab(&quot;\nControl Outcome&quot;) # \n adds a line break before the label</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<hr />
<p>Ok, we‚Äôve simulated one run of the control group. Now let‚Äôs think about the treatment group. Imagine we anticipate that a realistic effect of the treatment‚Äìagain based on prior knowledge‚Äìis an average relative improvement of 1%. We simulate this with:</p>
<pre class="r"><code>rel_effect &lt;- 0.01
treat_prob &lt;- control_prob + (control_prob * rel_effect)
treatment_y &lt;- rbinom(n = n_obs, size = 1, prob = treat_prob)

hist(treatment_y)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Finally, we compare the means of the two groups:</p>
<pre class="r"><code>mean(treatment_y) - mean(control_y) </code></pre>
<pre><code>## [1] 0.00102</code></pre>
<p>That‚Äôs one simulation. Let‚Äôs wrap it up in a function so it will be easy to run <span class="math inline">\(n\)</span> simulations</p>
<pre class="r"><code>one_sim &lt;- function(n, control_prob, rel_effect) {
  treat_prob &lt;- control_prob + (control_prob * rel_effect)
    
  cy &lt;- rbinom(n = n, size = 1, prob = control_prob)
  ty &lt;- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}</code></pre>
<p>Let‚Äôs test it to make sure it works.</p>
<pre class="r"><code>one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)</code></pre>
<pre><code>## [1] 0.001022</code></pre>
<p>This is very close to the true increase of <span class="math inline">\(\frac{\bar{x}_{treat} - \bar{x}_{control}}{\bar{x}_{control}}\cdot100=1\%\)</span>.</p>
<hr />
</div>
</div>
<div id="documenting" class="section level2">
<h2>Documenting Functions in R</h2>
<p>A quick aside: it is important to get in the habit of documenting your functions as you write them. This makes them easier for others (and yourself in the future) to understand.</p>
<p>There are some R documentation conventions to follow.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Function documentation begins with a description of what the function does. Then each function parameter is documented using <code>@param</code>. You can also give examples of the function working with <code>@examples</code>. All of the documentation should be commented out with <code>#'</code>. E.g.</p>
<pre class="r"><code>#&#39; Simulate one A/B test outcome from the binomial distribution
#&#39; and return the difference of the means of the two treatment arms.
#&#39; 
#&#39; @param n integer number of observations per treatment arm
#&#39; @param control_prob numeric probability of success in the control group
#&#39; @param rel_effect numeric relative effect of the treatment compared
#&#39; to the control

one_sim &lt;- function(n, control_prob, rel_effect) {
  treat_prob &lt;- control_prob + (control_prob * rel_effect)
    
  cy &lt;- rbinom(n = n, size = 1, prob = control_prob)
  ty &lt;- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}</code></pre>
<hr />
<div id="monte-carlo-with-for" class="section level3">
<h3>Monte Carlo with <code>for</code></h3>
<pre class="r"><code># package to add a progress bar to monitor loop progress
xfun::pkg_attach2(&quot;progress&quot;)

# Create vector of NAs to take the simulation results
n_sims &lt;- 100
mean_diff &lt;- rep(NA, n_sims)
pb &lt;- progress_bar$new(total = n_sims)

for (i in seq_along(mean_diff)) {
  pb$tick() # increment the progress bar
  mean_diff[i] &lt;- one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)
}

hist(mean_diff)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="monte-carlo-with-apply-and-parallelization" class="section level3">
<h3>Monte Carlo with <code>apply</code> and parallelization</h3>
<p><code>for</code> loops are <a href="https://privefl.github.io/blog/why-loops-are-slow-in-r/">notoriously slow in R</a>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This isn‚Äôt a problem if you are only doing a few simulations, but gets very tedious if you need to run many simulations. Often‚Äìthough not always‚Äìusing the <code>apply</code> family of functions is faster. They make it easier to parallelize your code.</p>
<p>Since we want to run the same function <code>n</code> times, we can use the <code>replicate()</code> helper function:<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<pre class="r"><code>system.time(
  mean_diff &lt;- replicate(n_sims, one_sim(n = n_obs, 
                                       control_prob = control_prob, 
                                       rel_effect = rel_effect))
)</code></pre>
<pre><code>##    user  system elapsed 
##  14.342   0.229  16.045</code></pre>
<p>I wrapped this call in <code>system.time()</code> so that we can keep track of how much computation time we spend on the call. Monte Carlo simulations are just doing the same thing over and over. This computation time can add up.</p>
<p>We can see that the the <code>apply</code> version in this case is pretty much as slow as the <code>for</code> loop.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<pre class="r"><code>system.time(
  for (i in seq_along(mean_diff)) {
    mean_diff[i] &lt;- one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)
  }
)</code></pre>
<pre><code>##    user  system elapsed 
##  14.128   0.139  14.642</code></pre>
<p>So far we have been running all of our simulations in sequence. However, each simulation, by construction, is independent of the others. This is a perfect use case for parallelization.</p>
<p>There is a <code>mc_replicate()</code> helper function (originally from the rethinking package) that makes this easy for the code we have written so far. <em>Note more than 1 core will not work on Windows.</em></p>
<pre class="r"><code># Run the following code if mcreplicate is not already installed
# xfun::pkg_attach2(&quot;remotes&quot;)
# install_github(&quot;christophergandrud/mcreplicate&quot;)

library(mcreplicate)

system.time(
  mc_replicate(n_sims, one_sim(n = n_obs, 
                              control_prob = control_prob, 
                              rel_effect = rel_effect),
              mc.cores = 4)
)</code></pre>
<pre><code>##    user  system elapsed 
##  10.389   0.250   4.218</code></pre>
<p>As we would expect from running the simulations across 4 rather than one core, the <code>elapsed</code> time is 4 times faster with <code>mc_replicate()</code>.</p>
<p>Finally, let‚Äôs see if the difference of means is a useful way of estimating the treatment effect for a binary outcome? Let‚Äôs run a few more simulations. We should see normally distributed difference of means centered on 0.001:</p>
<pre class="r"><code>diff_means_1000 &lt;- mc_replicate(1e3, one_sim(n = n_obs, 
                                      control_prob = control_prob, 
                                      rel_effect = rel_effect),
                              mc.cores = 4)</code></pre>
<pre class="r"><code># find mean of simulations
mean_of_means &lt;- mean(diff_means_1000)

# plot simulations with line at mean value
diff_means_1000_tbl &lt;- as_tibble(diff_means_1000)

ggplot(diff_means_1000_tbl, aes(value)) +
  geom_histogram(alpha = 0.6, fill = &quot;#29ffc6&quot;) +
  geom_vline(xintercept = mean_of_means, linetype = &quot;dashed&quot;) +
  xlab(&quot;\nTreatment - Control&quot;) +
  ggtitle(&quot;Monte Carlo Results&quot;, 
          subtitle = &quot;Difference of means for binary outcome\n10,000 simulations&quot;)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>The more simulations we run, the more smooth and ‚Äúnormal‚Äù the curve will become.</p>
</div>
</div>
<div id="debugging-intro" class="section level2">
<h2>Debugging in R</h2>
<p>Debugging is the process of finding and resolving discrepancies between a program‚Äôs specification and its implementation. For example, I want a ggplot2 to plot multiple time series on the same plot as separate lines, but it plots them all as one‚Äìvery jagged and confusing line. I use a debugging process to find the cause of and correct this discrepancy.</p>
<p>The reason we are covering debugging now, is that statistical simulations are more complex processes and debugging them is more complicated and crucial.</p>
<p><a href="https://en.wikipedia.org/wiki/Debugging"><img src="https://upload.wikimedia.org/wikipedia/commons/f/ff/First_Computer_Bug%252C_1945.jpg" title="An actual computer bug (source Wikipedia)" /></a></p>
<div id="being-mindful-of-your-feelings" class="section level3">
<h3>Being mindful of your feelings</h3>
<p>We‚Äôll cover the the technical debugging tools available in R. But I first want to first mention a crucial part of effective (and less stressful) debugging: <em>being mindful of your feelings about the bug</em>.</p>
<p>A bug is frustrating because we are experiencing an, initially unclear, disassociation between who we think the world should work and how it is working. It‚Äôs as if we want to put a plate in the cupboard, but each time we try this it flies out. Frustration inclines us to look for the cause of a bug in‚Äìalmost always‚Äìthe wrong place: ‚Äúthe stupid computer is broken.‚Äù</p>
<p>The best way to resolve bugs, as we‚Äôll see, is to create testable hypotheses about what could be causing the bug. ‚ÄúThe stupid computer is broken‚Äù is vague and so not a testable hypothesis.</p>
<p>So, it is crucial that when you find a bug, to take a step back and ask:</p>
<blockquote>
<p>how does this bug make me feel?</p>
</blockquote>
<p>If the answer is ‚Äúangry, I want to throw my computer out the window,‚Äù your best next step is to <em>take a break</em>. Grab a tea or whatever. Then, with a piece of paper (or some other tool that gets you away from the temptation to just begin slamming out code on your keyboard) write down hypotheses about the cause of the bug and ways to test these.</p>
<p>Unlike social phenomena, most computer bugs are deterministic. The cause of a bug will always cause the bug. So your chances of definitively finding the bug are high with the following process and tools.</p>
</div>
<div id="clear-testable-program-specifications" class="section level3">
<h3>Clear, testable program specifications</h3>
<blockquote>
<p>Finding your bug is a process of confirming the many things that you believe are true ‚Äî until you find one which is not true.</p>
<p>‚ÄîNorm Matloff (quote from <a href="https://adv-r.hadley.nz/debugging.html#debugging-strategy">Advanced R</a>)</p>
</blockquote>
<p>Great debugging is driven by having great expectations of what the program should return. Having a well defined expectation of what a program should do is important to knowing if you have a bug to begin with. One way to approach software development is to even write tests for a function before you write the function. This is called <a href="https://en.wikipedia.org/wiki/Test-driven_development">test drive development</a>.</p>
<p>Both in your functions, to test incremental expectations, and for the output of your functions you include automated tests. Here is a silly example. Imagine that we have a function that should add 2 + 2 and then divide by 2 to return 2. We can create an automated test like this:</p>
<pre class="r"><code>two &lt;- function() {
  four &lt;- 2 + 2
  if (four != 4) 
    stop(&quot;Did not add 2 + 2 correctly&quot;)
  
  four / 2
}

if (two() != 2) stop(&quot;function two did not return two&quot;)</code></pre>
<p>The <code>if</code> . . . <code>stop</code> statements automatically test whether or not the function is executing correctly. If there is a problem, the tests should help us pinpoint the cause more quickly.</p>
</div>
<div id="browser-for-testing-and-dissecting" class="section level3">
<h3><code>browser()</code> for testing and dissecting</h3>
<p>Imagine that our function is not working correctly. A key R tool is the <code>browser()</code> function. When R executes some code and hits <code>browser()</code> it stops the execution and begins an interactive <code>Browse</code> mode. This allows you to explore the objects in the current environment and run code. RStudio works really well with this function.</p>
<p>Imagine we wrote the loop above incorrectly:</p>
<pre class="r"><code>one_sim_bug &lt;- function(n, control_prob, rel_effect) {
  treat_prob &lt;- control_prob / (control_prob * rel_effect)
    
  cy &lt;- rbinom(n = n, size = 1, prob = control_prob)
  ty &lt;- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}

mean_diff_bug &lt;- rep(NA, n_sims)

for (i in seq_along(mean_diff)) {
  mean_diff_bug[u] &lt;- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}</code></pre>
<p>We two layers of nested functions where the bug could reside. Let‚Äôs systematically use <code>browser()</code> to identify the source. Start from the outer most function‚Äìthe <code>for</code> loop and work you way down as necessary.</p>
<pre class="r"><code>for (i in seq_along(mean_diff)) {
browser()
  mean_diff_bug[u] &lt;- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}</code></pre>
<p>Ok, we‚Äôve found the a typo. We should have used <code>i</code> instead of <code>u</code> to index the output of <code>one_sim</code>. Leave the browser environment with <code>c + Enter</code>. Are we done? We‚Äôll let‚Äôs check the output:</p>
<pre class="r"><code>for (i in seq_along(mean_diff)) {
  mean_diff_bug[i] &lt;- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}

hist(mean_diff_bug)</code></pre>
<p>Hm, we are not getting a valid histogram. Let‚Äôs now go down the function nesting to insert a <code>browser()</code> call in <code>one_sim</code>:</p>
<pre class="r"><code>one_sim_bug &lt;- function(n, control_prob, rel_effect) {
  browser()
  treat_prob &lt;- control_prob / (control_prob * rel_effect)
    
  cy &lt;- rbinom(n = n, size = 1, prob = control_prob)
  ty &lt;- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}

mean_diff_bug &lt;- rep(NA, n_sims)

for (i in seq_along(mean_diff)) {
  mean_diff_bug[u] &lt;- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}</code></pre>
<p>When we do this, we can find that we incorrectly divide <code>control_prob</code> by <code>(control_prob * rel_effect)</code> the ultimate result, is that the output of <code>one_sim</code> is <code>NA</code>. We can‚Äôt create a histogram of <code>NA</code>s.</p>
</div>
</div>
<div id="exercise" class="section level2">
<h2>ü•Ö Exercise</h2>
<p><strong>A.</strong> Replicate the example above. This time use Monte Carlo simulation to find out the distribution of p-values if the you had a <em>total</em> sample size of 2,000. Hint: extract p-values in R from a t-test with <code>t.test()$p.value</code> .</p>
<p>‚ÄúExtra credit‚Äù: how often you would incorrectly reject the null hypothesis of no difference of means at the 95% level with <em>total</em> sample sizes of 1,000, 2,000, 10,000, and 100,000? Plot the proportion of simulations falsely rejected across these sample sizes.</p>
<p><strong>B.</strong> Imagine that we are interested in understanding how bad <a href="https://en.wikipedia.org/wiki/Confounding">confounding</a> by an unobserved variable:</p>
<div id="htmlwidget-2" style="width:672px;height:480px;" class="DiagrammeR html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"diagram":"\ngraph LR\n  Z-->Y\n  Z-->X\n  X-->Y\n"},"evals":[],"jsHooks":[]}</script>
<p><span class="math inline">\(Z\)</span> is a confounder. It causes both <span class="math inline">\(X\)</span> and our outcome of interest <span class="math inline">\(Y\)</span>. Imagine we couldn‚Äôt observe <span class="math inline">\(Z\)</span>. Create a Monte Carlo simulation to explore how this confounder could be biasing our estimate of the relationship between <span class="math inline">\(X\)</span> and our <span class="math inline">\(Y\)</span>.</p>
<p>Assume a data generating process described by:</p>
<p><span class="math display">\[
Z \sim \mathrm{Binomial}(0.5)\\
X \sim \mathrm{Binomial}(0.8 - 0.6 \cdot Z) \\
\epsilon \sim \mathrm{Normal}(0, 1) \\
Y = X \cdot 1 + Z \cdot 3 + \epsilon
\]</span></p>
<p>How different is the estimated effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> when we include the unobserved confounder compared to when we don‚Äôt include it?</p>
<p>Hints: use <code>lm()</code> to estimate a linear model. Coefficients can be extracted from <code>lm</code> model objects, say <code>m1</code> with <code>m1$coefficients</code>.</p>
<p>Extra credit: how does this impact change as you vary the relationship between <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> i.e.¬†change <span class="math inline">\(0.6 \cdot Z\)</span>? Answering this question will likely take some programming beyond what we have covered so far. Google is your friend here.</p>
<p><strong>C.</strong> Propose a data generating process and estimation strategy related to work that you are interested in. Conduct Monte Carlo analysis of the estimation strategy given this data generating process.</p>
<p><strong>Completely over to you:</strong> Now propose a data generating process and estimation strategy related to work that you are interested in. Conduct Monte Carlo analysis of the estimation strategy given this data generating process.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gelmanhillaki2021" class="csl-entry">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. <em>Regression and Other Stories</em>. Cambridge: Cambridge University Press.
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This example is modified from <span class="citation"><a href="#ref-gelmanhillaki2021" role="doc-biblioref">Gelman, Hill, and Vehtari</a> (<a href="#ref-gelmanhillaki2021" role="doc-biblioref">2021</a> Ch. 5, p.69-70)</span>.<a href="#fnref1" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>We can express this in R using scientific notation, e.g.¬†<code>2e6</code>.<a href="#fnref2" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>These conventions are established by the <a href="https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html">roxygen2</a> package. If you create an R package to contain your functions, you can use roxygen2 to automatically generate package documentation. For more details see <a href="https://r-pkgs.org/">R Packages</a>.<a href="#fnref3" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p>If you ever need to kill an R process (e.g.¬†because it is taking way longer than you expected, use <code>Ctrl + c</code> in your R console.<a href="#fnref4" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn5"><p><code>replicate()</code> is a wrapper for <code>sapply()</code> you can see how this code works by typing <code>sapply</code> into your R console.<a href="#fnref5" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn6"><p>Note that there is random variation in computation time. So if you want to get a more accurate sense of computation time use the <a href="https://cran.r-project.org/web/packages/microbenchmark/microbenchmark.pdf">microbenchmark</a> package. This does repeated runs of the call and reports the distribution of run times.<a href="#fnref6" class="footnote-back">‚Ü©Ô∏é</a></p></li>
</ol>
</div>
