---
title: '[Intro to Computational Thinking] Introduction to Computational Thinking via Simulation with R'
author: Christopher Gandrud
date: '2021-03-17'
slug: optional-r-basics-3
weight: 4
categories:
  - R
  - install
tags: [R, simulation, distributed, functions]
Categories: []
Description: ''
Tags: [R, simulation, distributed, functions]
bibliography: main.bib
---

> "What I cannot create, I do not understand" -- [Richard Feynman](https://archives.caltech.edu/pictures/1.10-29.jpg)

> "It is immoral to collect data before simulating the hypothesized data generating process." -- Andrew Gelman (what I remember of a comment to the Stanford Causal Inference Seminar in 2020).

```{r echo=FALSE}
# Load required packages
xfun::pkg_attach2("DiagrammeR")
```

## 📝 Lesson Preview

-   [Why simulate?](#motivation)

-   [Simple simulation workflow](#workflow)

-   [Documenting Functions in RMonte Carlo with apply and parallelization](#documenting)

-   [Debugging in R](debugging-intro)

-   Throughout, we will include examples of how to plot with base R and ggplot2

## Motivation: Why simulate? {#motivation}

We've covered many of the basics of programming in R. Let's bring many of these concepts together to help us better understand our research problems, even before we gather any data. Let's simulate some **data generating processes (DGP)**.

A data generating process is the process that creates the data in a data set. These processes can include, for example, both the social processes that result in migration data and even unexpected (or undesired) issues in the sampling procedure like non-random study attrition.

Wait, why are we talking about simulating data generating processes in an introduction to R course? There are two sets of reasons.

First, computational generative modeling--simulating data generating processes with a computer--can be a key tool for understanding and evaluating the statistical methods you are using to understand a given data generating process [@gelmanhillaki2021, p. 76]. Will your chosen statistical model identify the effect you are interested, even if you completely understand the DGP? This is surprisingly often not the case, especially for non-trivial data generating processes. Simulation allows you to stress test your chosen models, with a given assumed DGP, and compare the appropriateness of different models for this DGP.

In Gelman's strong view stated above, if your identification strategy can't recover effects in known--simulated data--you shouldn't waste time and money collecting real world data.

The second reason we are covering statistical simulation, and most importantly for this class, is that simulation requires you to work your new skills as an R programmer. Because the DGP you are working with are probabilistic, you will need to repeat the simulations with the same data generating process to understand the statistical properties. This is called the [Monte Carlo Method](https://en.wikipedia.org/wiki/Monte_Carlo_method). So you will need to write functions, run them efficiently, e.g. sometimes in parallel, transform large amounts of data, and plot the results.

## Simple simulation workflow {#workflow}

A typical simulation workflow looks something like this:

```{r sim-workflow, echo=FALSE}
DiagrammeR::mermaid("
graph TB
  A[Hypothesise DGP]-->AA[Identification strategies]
  AA-->B[Functions for 1 draw & analysis]
  B-->C[Function to repeat for n draws]
  B-- Debug -->B
  C-->D[Plot simulations]
  C-- Debug -->C 
  D-- Debug -->D
  D-- Iterate -->A
")
```

We start by hypothesizing what the data generating process we care about might be. Then we propose one or candidate strategies to identify effects in this data generating process, e.g. regression, matching, difference in differences. We begin implementing the simulation fo this DGP-identification strategy by creating function(s) for simulating and analyzing the data for one simulation. Once we have the simulation correctly working for one draw, we create a function(s) to make $n$ draws. We then (usually) examine the results by plotting quantities of interest from the simulations, such as the distribution of some estimate or the error from some ground truth across simulations. Frequently, we learn something unexpected by the end either about our understanding of the data generating process or our candidate identification strategies. So we often need to repeat the process.

Notice that most steps often involve some debugging. We'll dig debugging into that [below](#debugging-intro). But let's look at a simple ideal case.

### Example 1: Simple A/B Test Discrete probabilities

Imagine that we want to run an A/B test--randomised control trial--a new treatment to the status quo control treatment. We are interested in the effect of the treatment on whether or not individuals in the study do some action--e.g. vote in an election.[^1] This is a discrete variable. Can we use a difference of means test to estimate the treatment effect?

[^1]: This example is modified from @gelmanhillaki2021 [ Ch. 5, p.69-70].

Let's start by hypothesizing what the data generating process could be. Imagine that based on prior research we know that their is a baseline probability of the action with the control of 0.1:

$$Y \sim \mathrm{Binomial}(0.1)$$ To simulate one draw from this binomial distribution in R use:

```{r}
control_y_1 <- rbinom(n = 1, size = 1, prob = 0.1)
control_y_1
```

Let's walk through this code. `rbinom()` is the function to randomly (`r`) draw from the binomial distribution (`binom`). `n` is the number of draws for this simulation, `size` is the "number of trials", basically it means that you want to the "success" outcome to be 1 and 0 otherwise. `prob` is the probability of "success".

It's unlikely that we will run an A/B test with only one participant in the control group, let's go up to 2,000,000[^2] and plot the results.

[^2]: We can express this in R using scientific notation, e.g. `2e6`.

```{r}
# number of observations variable to ensure consistency across sims
n_obs <- 2e6
control_prob <- 0.1

control_y <- rbinom(n = n_obs, size = 1, prob = control_prob)
hist(control_y)
```

------------------------------------------------------------------------

### Side note: Histograms with ggplot2

[ggplot2](https://ggplot2.tidyverse.org/) is a very powerful way to create graphics in R. Here is an example of how you would create a histogram with ggplot2.

```{r message=FALSE, warning=FALSE}
# ggplot2 is a member of the "tidyverse" set of packages
xfun::pkg_attach2("tidyverse")

# I personally enjoy the simplicity of the linedraw theme
theme_set(theme_linedraw())

# ggplot cannot accept a vector, like hist. 
# Instead you need to convert to a data frame or the tidyverse equivalent "tibble"
control_y_tbl <- as_tibble(control_y)

# Plot and change the x-axis label
ggplot(control_y_tbl, aes(value)) + # as_tibble names the column "value" by default
  geom_histogram() + 
  xlab("\nControl Outcome") # \n adds a line break before the label
```

------------------------------------------------------------------------

Ok, we've simulated one run of the control group. Now let's think about the treatment group. Imagine we anticipate that a realistic effect of the treatment--again based on prior knowledge--is an average relative improvement of 1%. We simulate this with:

```{r}
rel_effect <- 0.01
treat_prob <- control_prob + (control_prob * rel_effect)
treatment_y <- rbinom(n = n_obs, size = 1, prob = treat_prob)

hist(treatment_y)
```

Finally, we compare the means of the two groups:

```{r}
mean(treatment_y) - mean(control_y) 
```

That's one simulation. Let's wrap it up in a function so it will be easy to run $n$ simulations

```{r}
one_sim <- function(n, control_prob, rel_effect) {
  treat_prob <- control_prob + (control_prob * rel_effect)
    
  cy <- rbinom(n = n, size = 1, prob = control_prob)
  ty <- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}
```

Let's test it to make sure it works.

```{r}
one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)
```

This is very close to the true increase of $\frac{\bar{x}_{treat} - \bar{x}_{control}}{\bar{x}_{control}}\cdot100=1\%$.

------------------------------------------------------------------------

## Documenting Functions in R {#documenting}

A quick aside: it is important to get in the habit of documenting your functions as you write them. This makes them easier for others (and yourself in the future) to understand.

There are some R documentation conventions to follow.[^3] Function documentation begins with a description of what the function does. Then each function parameter is documented using `@param`. You can also give examples of the function working with `@examples`. All of the documentation should be commented out with `#'`. E.g.

[^3]: These conventions are established by the [roxygen2](https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html) package. If you create an R package to contain your functions, you can use roxygen2 to automatically generate package documentation. For more details see [R Packages](https://r-pkgs.org/).

```{r eval=FALSE}
#' Simulate one A/B test outcome from the binomial distribution
#' and return the difference of the means of the two treatment arms.
#' 
#' @param n integer number of observations per treatment arm
#' @param control_prob numeric probability of success in the control group
#' @param rel_effect numeric relative effect of the treatment compared
#' to the control

one_sim <- function(n, control_prob, rel_effect) {
  treat_prob <- control_prob + (control_prob * rel_effect)
    
  cy <- rbinom(n = n, size = 1, prob = control_prob)
  ty <- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}
```

------------------------------------------------------------------------

### Monte Carlo with `for`

```{r message=FALSE}
# package to add a progress bar to monitor loop progress
xfun::pkg_attach2("progress")

# Create vector of NAs to take the simulation results
n_sims <- 100
mean_diff <- rep(NA, n_sims)
pb <- progress_bar$new(total = n_sims)

for (i in seq_along(mean_diff)) {
  pb$tick() # increment the progress bar
  mean_diff[i] <- one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)
}

hist(mean_diff)
```

### Monte Carlo with `apply` and parallelization

`for` loops are [notoriously slow in R](https://privefl.github.io/blog/why-loops-are-slow-in-r/).[^4] This isn't a problem if you are only doing a few simulations, but gets very tedious if you need to run many simulations. Often--though not always--using the `apply` family of functions is faster. They make it easier to parallelize your code.

[^4]: If you ever need to kill an R process (e.g. because it is taking way longer than you expected, use `Ctrl + c` in your R console.

Since we want to run the same function `n` times, we can use the `replicate()` helper function:[^5]

[^5]: `replicate()` is a wrapper for `sapply()` you can see how this code works by typing `sapply` into your R console.

```{r cache=TRUE}
system.time(
  mean_diff <- replicate(n_sims, one_sim(n = n_obs, 
                                       control_prob = control_prob, 
                                       rel_effect = rel_effect))
)
```

I wrapped this call in `system.time()` so that we can keep track of how much computation time we spend on the call. Monte Carlo simulations are just doing the same thing over and over. This computation time can add up.

We can see that the the `apply` version in this case is pretty much as slow as the `for` loop.[^6]

[^6]: Note that there is random variation in computation time. So if you want to get a more accurate sense of computation time use the [microbenchmark](https://cran.r-project.org/web/packages/microbenchmark/microbenchmark.pdf) package. This does repeated runs of the call and reports the distribution of run times.

```{r cache=TRUE}
system.time(
  for (i in seq_along(mean_diff)) {
    mean_diff[i] <- one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)
  }
)
```

So far we have been running all of our simulations in sequence. However, each simulation, by construction, is independent of the others. This is a perfect use case for parallelization.

There is a `mc_replicate()` helper function (originally from the rethinking package) that makes this easy for the code we have written so far. *Note more than 1 core will not work on Windows.*

```{r cache=TRUE}
# Run the following code if mcreplicate is not already installed
# xfun::pkg_attach2("remotes")
# install_github("christophergandrud/mcreplicate")

library(mcreplicate)

system.time(
  mc_replicate(n_sims, one_sim(n = n_obs, 
                              control_prob = control_prob, 
                              rel_effect = rel_effect),
              mc.cores = 4)
)
```

As we would expect from running the simulations across 4 rather than one core, the `elapsed` time is 4 times faster with `mc_replicate()`.

Finally, let's see if the difference of means is a useful way of estimating the treatment effect for a binary outcome? Let's run a few more simulations. We should see normally distributed difference of means centered on 0.001:

```{r cache=TRUE, message=FALSE}
diff_means_1000 <- mc_replicate(1e3, one_sim(n = n_obs, 
                                      control_prob = control_prob, 
                                      rel_effect = rel_effect),
                              mc.cores = 4)

# find mean of simulations
mean_of_means <- mean(diff_means_1000)

# plot simulations with line at mean value
diff_means_1000_tbl <- as_tibble(diff_means_1000)

ggplot(diff_means_1000_tbl, aes(value)) +
  geom_histogram(alpha = 0.6, fill = "#29ffc6") +
  geom_vline(xintercept = mean_of_means, linetype = "dashed") +
  xlab("\nTreatment - Control") +
  ggtitle("Monte Carlo Results", 
          subtitle = "Difference of means for binary outcome\n10,000 simulations")
```

The more simulations we run, the more smooth and "normal" the curve will become.

## Debugging in R {#debugging-intro}

Debugging is the process of finding and resolving discrepancies between a program's specification and its implementation. For example, I want a ggplot2 to plot multiple time series on the same plot as separate lines, but it plots them all as one--very jagged and confusing line. I use a debugging process to find the cause of and correct this discrepancy.

The reason we are covering debugging now, is that statistical simulations are more complex processes and debugging them is more complicated and crucial.

[![](https://upload.wikimedia.org/wikipedia/commons/f/ff/First_Computer_Bug%252C_1945.jpg "An actual computer bug (source Wikipedia)")](https://en.wikipedia.org/wiki/Debugging)

### Being mindful of your feelings

We'll cover the the technical debugging tools available in R. But I first want to first mention a crucial part of effective (and less stressful) debugging: *being mindful of your feelings about the bug*.

A bug is frustrating because we are experiencing an, initially unclear, disassociation between who we think the world should work and how it is working. It's as if we want to put a plate in the cupboard, but each time we try this it flies out. Frustration inclines us to look for the cause of a bug in--almost always--the wrong place: "the stupid computer is broken".

The best way to resolve bugs, as we'll see, is to create testable hypotheses about what could be causing the bug. "The stupid computer is broken" is vague and so not a testable hypothesis.

So, it is crucial that when you find a bug, to take a step back and ask:

> how does this bug make me feel?

If the answer is "angry, I want to throw my computer out the window", your best next step is to *take a break*. Grab a tea or whatever. Then, with a piece of paper (or some other tool that gets you away from the temptation to just begin slamming out code on your keyboard) write down hypotheses about the cause of the bug and ways to test these.

Unlike social phenomena, most computer bugs are deterministic. The cause of a bug will always cause the bug. So your chances of definitively finding the bug are high with the following process and tools.

### Clear, testable program specifications

> Finding your bug is a process of confirming the many things that you believe are true --- until you find one which is not true.
>
> ---Norm Matloff (quote from [Advanced R](https://adv-r.hadley.nz/debugging.html#debugging-strategy))

Great debugging is driven by having great expectations of what the program should return. Having a well defined expectation of what a program should do is important to knowing if you have a bug to begin with. One way to approach software development is to even write tests for a function before you write the function. This is called [test drive development](https://en.wikipedia.org/wiki/Test-driven_development).

Both in your functions, to test incremental expectations, and for the output of your functions you include automated tests. Here is a silly example. Imagine that we have a function that should add 2 + 2 and then divide by 2 to return 2. We can create an automated test like this:

```{r}
two <- function() {
  four <- 2 + 2
  if (four != 4) 
    stop("Did not add 2 + 2 correctly")
  
  four / 2
}

if (two() != 2) stop("function two did not return two")
```

The `if` . . . `stop` statements automatically test whether or not the function is executing correctly. If there is a problem, the tests should help us pinpoint the cause more quickly.

### `browser()` for testing and dissecting

Imagine that our function is not working correctly. A key R tool is the `browser()` function. When R executes some code and hits `browser()` it stops the execution and begins an interactive `Browse` mode. This allows you to explore the objects in the current environment and run code. RStudio works really well with this function.

Imagine we wrote the loop above incorrectly:

```{r eval=FALSE}
one_sim_bug <- function(n, control_prob, rel_effect) {
  treat_prob <- control_prob / (control_prob * rel_effect)
    
  cy <- rbinom(n = n, size = 1, prob = control_prob)
  ty <- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}

mean_diff_bug <- rep(NA, n_sims)

for (i in seq_along(mean_diff)) {
  mean_diff_bug[u] <- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}
```

We two layers of nested functions where the bug could reside. Let's systematically use `browser()` to identify the source. Start from the outer most function--the `for` loop and work you way down as necessary.

```{r eval=FALSE}
for (i in seq_along(mean_diff)) {
browser()
  mean_diff_bug[u] <- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}
```

Ok, we've found the a typo. We should have used `i` instead of `u` to index the output of `one_sim`. Leave the browser environment with `c + Enter`. Are we done? We'll let's check the output:

```{r eval=FALSE}
for (i in seq_along(mean_diff)) {
  mean_diff_bug[i] <- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}

hist(mean_diff_bug)
```

Hm, we are not getting a valid histogram. Let's now go down the function nesting to insert a `browser()` call in `one_sim`:

```{r eval=FALSE}
one_sim_bug <- function(n, control_prob, rel_effect) {
  browser()
  treat_prob <- control_prob / (control_prob * rel_effect)
    
  cy <- rbinom(n = n, size = 1, prob = control_prob)
  ty <- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}

mean_diff_bug <- rep(NA, n_sims)

for (i in seq_along(mean_diff)) {
  mean_diff_bug[u] <- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}
```

When we do this, we can find that we incorrectly divide `control_prob` by `(control_prob * rel_effect)` the ultimate result, is that the output of `one_sim` is `NA`. We can't create a histogram of `NA`s.

## 🥅 Exercise

**A.** Replicate the example above. This time use Monte Carlo simulation to find out the distribution of p-values if the you had a *total* sample size of 2,000. Hint: extract p-values in R from a t-test with `t.test()$p.value` .

"Extra credit": how often you would incorrectly reject the null hypothesis of no difference of means at the 95% level with *total* sample sizes of 1,000, 2,000, 10,000, and 100,000? Plot the proportion of simulations falsely rejected across these sample sizes.

**B.** Imagine that we are interested in understanding how bad [confounding](https://en.wikipedia.org/wiki/Confounding) by an unobserved variable:

```{r confounding, echo=FALSE}
DiagrammeR::mermaid("
graph LR
  Z-->Y
  Z-->X
  X-->Y
")
```

$Z$ is a confounder. It causes both $X$ and our outcome of interest $Y$. Imagine we couldn't observe $Z$. Create a Monte Carlo simulation to explore how this confounder could be biasing our estimate of the relationship between $X$ and our $Y$.

Assume a data generating process described by:

$$
Z \sim \mathrm{Binomial}(0.5)\\
X \sim \mathrm{Binomial}(0.8 - 0.6 \cdot Z) \\
\epsilon \sim \mathrm{Normal}(0, 1) \\
Y = X \cdot 1 + Z \cdot 3 + \epsilon
$$

How different is the estimated effect of $X$ on $Y$ when we include the unobserved confounder compared to when we don't include it?

Hints: use `lm()` to estimate a linear model. Coefficients can be extracted from `lm` model objects, say `m1` with `m1$coefficients`.

Extra credit: how does this impact change as you vary the relationship between $Z$ and $X$ i.e. change $0.6 \cdot Z$? Answering this question will likely take some programming beyond what we have covered so far. Google is your friend here.

**C.** Propose a data generating process and estimation strategy related to work that you are interested in. Conduct Monte Carlo analysis of the estimation strategy given this data generating process.

## References
